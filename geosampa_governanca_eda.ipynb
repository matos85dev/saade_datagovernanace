{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69127518-2a82-40ea-9e58-5d74eb7f7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66151da-dce9-4fe6-95a1-db20711e2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import httpx\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "from tenacity import before_sleep_log, retry, retry_if_exception_type, stop_after_attempt, wait_random_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1ddbbd-c05d-4821-a9aa-4a1047a8ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95283d5d-dc8d-4432-a10f-b02b062654bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import HTTPError\n",
    "\n",
    "class ResponseNotJson(HTTPError):\n",
    "    '''Raised when the response is not a JSON'''\n",
    "\n",
    "class ResponseNotXML(HTTPError):\n",
    "    '''Raised when the response is not a XML'''\n",
    "\n",
    "class PaginationError(HTTPError):\n",
    "    '''Raised hen total retrieved features is not equal to total features returned by API'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f8951a-7c4e-48e3-a2c2-646e92054d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_for_status(func):\n",
    "    '''Raises HTTP error for response status codes 4xx or 5xx.\n",
    "    If not, returns content of response'''\n",
    "\n",
    "    def decorated(*args, **kwargs):\n",
    "\n",
    "        response = func(*args, **kwargs)\n",
    "        response.raise_for_status()\n",
    "        content = response.content\n",
    "\n",
    "        return content\n",
    "\n",
    "    return decorated\n",
    "\n",
    "\n",
    "def json_response(func):\n",
    "    '''Parses json response. Raises xml string error if\n",
    "    xml is returned'''\n",
    "\n",
    "    def decorated(*args, **kwargs):\n",
    "\n",
    "        response = func(*args, **kwargs)\n",
    "        json_txt = response.decode('utf-8')\n",
    "        try:\n",
    "            json_data = json.loads(json_txt)\n",
    "            return json_data\n",
    "        except JSONDecodeError:\n",
    "            response = xmltodict.parse(response)\n",
    "            raise ResponseNotJson(f'Response is not a JSON: {response}')\n",
    "    \n",
    "    return decorated\n",
    "\n",
    "def xml_response(func):\n",
    "    '''Parses xml response as a dict'''\n",
    "\n",
    "    def decorated(*args, **kwargs):\n",
    "        \n",
    "        try:\n",
    "            response = func(*args, **kwargs)\n",
    "            parsed = xmltodict.parse(response)\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            raise ResponseNotXML(f'XML parsing failed {e}')\n",
    "    \n",
    "    return decorated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a036ec-75de-4e49-aa66-970607122a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BaseClient:\n",
    "    \"\"\"WFS base client - used to make generic requests\"\"\"\n",
    "\n",
    "    accepted_versions = {\"2.0.0\"}\n",
    "    output_formats = {\"json\", \"xml\", \"bytes\"}\n",
    "\n",
    "    def __init__(self, domain: str, version: str = \"2.0.0\", verbose: bool = True):\n",
    "        self.domain = self.__clean_domain(domain)\n",
    "        self.version = self.__assert_version(version)\n",
    "        self.host = self.__gen_host()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __clean_domain(self, domain: str) -> str:\n",
    "        if domain.endswith(r\"/\"):\n",
    "            domain = domain[:-1]\n",
    "\n",
    "        return domain\n",
    "\n",
    "    def __assert_version(self, version: str) -> str:\n",
    "        if version not in self.accepted_versions:\n",
    "            raise ValueError(f\"Accepted versions: {self.accepted_versions}\")\n",
    "        return version\n",
    "\n",
    "    def __gen_host(self) -> str:\n",
    "        wfs_version = f\"/?service=WFS&version={self.version}\"\n",
    "        return self.domain + wfs_version\n",
    "\n",
    "    def __solve_get_params(self, *ignore, **query_params: dict) -> str:\n",
    "        request_args = [\"=\".join([param_name, str(param_value)]) for param_name, param_value in query_params.items()]\n",
    "        query_string = \"&\".join(request_args)\n",
    "\n",
    "        return query_string\n",
    "\n",
    "    def __solve_req_capability(self, capability: str) -> str:\n",
    "        capability_param = f\"request={capability}\"\n",
    "\n",
    "        return capability_param\n",
    "\n",
    "    def __solve_request_url(self, capability: str, **query_params: dict) -> str:\n",
    "        base_url = self.host\n",
    "        capability_param = self.__solve_req_capability(capability)\n",
    "        url = base_url + \"&\" + capability_param\n",
    "\n",
    "        if query_params is None:\n",
    "            return url\n",
    "        else:\n",
    "            req_params = self.__solve_get_params(**query_params)\n",
    "            return url + \"&\" + req_params\n",
    "\n",
    "    @retry(\n",
    "        retry=retry_if_exception_type(RequestException),\n",
    "        stop=stop_after_attempt(10),\n",
    "        wait=wait_random_exponential(5, min=5, max=60),\n",
    "        before_sleep=before_sleep_log(logger, logging.INFO, exc_info=True),\n",
    "    )\n",
    "    @raise_for_status\n",
    "    def wfs_generic_request(self, capability: str, *ignore, **query_params: dict) -> bytes:\n",
    "        url = self.__solve_request_url(capability, **query_params)\n",
    "\n",
    "        # response is in bytes, so must be decoed accordingly\n",
    "        with requests.get(url) as response:\n",
    "            return response\n",
    "\n",
    "    async def wfs_async_requests(self, capability: str, pages, **query_params: dict):\n",
    "        params = query_params.copy()\n",
    "        async with httpx.AsyncClient() as client:\n",
    "\n",
    "            @retry(\n",
    "                retry=retry_if_exception_type(httpx.HTTPError),\n",
    "                stop=stop_after_attempt(10),\n",
    "                wait=wait_random_exponential(30, min=30, max=300),\n",
    "                before_sleep=before_sleep_log(logger, logging.INFO, exc_info=True),\n",
    "            )\n",
    "            async def fetch(url, semaphore):\n",
    "                async with semaphore:\n",
    "                    response = await client.get(url, timeout=300)\n",
    "                    response.raise_for_status()\n",
    "                    return response\n",
    "\n",
    "            tasks = []\n",
    "            semaphore = asyncio.Semaphore(4)\n",
    "\n",
    "            for page in pages:\n",
    "                params[\"startIndex\"] = page\n",
    "                url = self.__solve_request_url(capability, **params)\n",
    "                task = asyncio.create_task(fetch(url, semaphore))\n",
    "                tasks.append(task)\n",
    "\n",
    "            responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "        for response in responses:\n",
    "            if isinstance(response, Exception):\n",
    "                # Handle exceptions\n",
    "                print(f\"Request failed: {response}\")\n",
    "\n",
    "        @json_response\n",
    "        def parse_response(response):\n",
    "            return response.content\n",
    "\n",
    "        responses = [parse_response(response) for response in responses]\n",
    "\n",
    "        first_response = responses[0]\n",
    "        for response in responses[1:]:\n",
    "            first_response[\"features\"].extend(response[\"features\"])\n",
    "\n",
    "        return first_response\n",
    "\n",
    "    @json_response\n",
    "    def get_json(self, capability: str, **query_params: dict) -> dict:\n",
    "        return self.wfs_generic_request(\n",
    "            capability, outputFormat=\"application/json\", exceptions=\"application/json\", **query_params\n",
    "        )\n",
    "\n",
    "    @xml_response\n",
    "    def get_xml(self, capability: str, **query_params: dict) -> dict:\n",
    "        return self.wfs_generic_request(capability, **query_params)\n",
    "\n",
    "    def __call__(self, capability: str, *ignore, output_format=\"json\", pages=None, **query_params: dict) -> bytes:\n",
    "        if output_format not in self.output_formats:\n",
    "            raise ValueError(f\"output_format must be in {self.output_formats}\")\n",
    "\n",
    "        if output_format == \"json\":\n",
    "            return self.get_json(capability, **query_params)\n",
    "\n",
    "        elif output_format == \"xml\":\n",
    "            return self.get_xml(capability, **query_params)\n",
    "\n",
    "        else:\n",
    "            return self.wfs_generic_request(capability, **query_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e8838c-42de-45ea-8fba-db28a36a31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class CQLFilter:\n",
    "    '''Builds a CQL filter for querying the WFS server'''\n",
    "    \n",
    "    def __init__(self, feature_name:str, schema:dict)->None:\n",
    "        \n",
    "        self.feature_name = feature_name\n",
    "        self.schema = schema[feature_name]\n",
    "        self.cql_filter = ''\n",
    "        \n",
    "    def add_to_filter(self, query_str:str, sep:str=';')->None:\n",
    "        \n",
    "        if len(self.cql_filter)<1:\n",
    "            self.cql_filter=query_str\n",
    "        else:\n",
    "            query_str = sep + query_str\n",
    "            self.cql_filter += query_str\n",
    "    \n",
    "    def __check_propertie_in_schema(self, prop_name:str)->None:\n",
    "        \n",
    "        if prop_name not in self.schema:\n",
    "            raise ValueError(f'Feature name {prop_name} must be in {self.schema.keys()}')\n",
    "        \n",
    "    def __propertie_equals(self, propertie_name:str, equals_to:Union[str, int, float]):\n",
    "        \n",
    "        self.__check_propertie_in_schema(propertie_name)\n",
    "        if type(equals_to) is str:\n",
    "            equals_to = f\"'{equals_to}'\"\n",
    "\n",
    "        query_str = f\"({propertie_name}={equals_to})\"\n",
    "        \n",
    "        return query_str\n",
    "    \n",
    "    def properties_equals(self, *ignore, **propertie_comparisons):\n",
    "        \n",
    "        #se tiver mais de um filtro, separa corretamente\n",
    "        self.add_to_filter('', sep=';')\n",
    "        for prop_name, prop_val in propertie_comparisons.items():\n",
    "            query_str = self.__propertie_equals(prop_name, prop_val)\n",
    "            self.add_to_filter(query_str, sep='AND')\n",
    "\n",
    "    def point_within_pol(self, x:float, y:float, precision:int=5)->dict:\n",
    "        '''Precision is in meters. Returns the polygon in self.feature_name \n",
    "        which intersects a buffer of {precision} meters centralized at the point'''\n",
    "\n",
    "        query = f'DWITHIN(ge_poligono,POINT({x} {y}),{precision},meters)'\n",
    "        self.add_to_filter(query)\n",
    "\n",
    "    def point_within_linha(self, x:float, y:float, precision:int=5)->dict:\n",
    "        '''Precision is in meters. Returns the polygon in self.feature_name \n",
    "        which intersects a buffer of {precision} meters centralized at the point'''\n",
    "\n",
    "        query = f'DWITHIN(ge_linha,POINT({x} {y}),{precision},meters)'\n",
    "        self.add_to_filter(query)\n",
    "\n",
    "    def point_within_multipol(self, x:float, y:float, precision:int=5)->dict:\n",
    "        '''Precision is in meters. Returns the polygon in self.feature_name \n",
    "        which intersects a buffer of {precision} meters centralized at the point'''\n",
    "\n",
    "        query = f'DWITHIN(ge_multipoligono,POINT({x} {y}),{precision},meters)'\n",
    "        self.add_to_filter(query)\n",
    "\n",
    "    def polygon_within_pol(self, coordinates:str, precision:int=5)->dict:\n",
    "        '''Precision is in meters. Returns the polygon in self.feature_name \n",
    "        which intersects a buffer of {precision} meters centralized at the point'''\n",
    "\n",
    "        query = f'DWITHIN(ge_poligono,POLYGON({coordinates}),{precision},meters)'\n",
    "        self.add_to_filter(query)\n",
    "\n",
    "    def polygon_within_linha(self, coordinates:str, precision:int=5)->dict:\n",
    "        '''Precision is in meters. Returns the polygon in self.feature_name \n",
    "        which intersects a buffer of {precision} meters centralized at the point'''\n",
    "\n",
    "        query = f'DWITHIN(ge_linha,POLYGON({coordinates}),{precision},meters)'\n",
    "        self.add_to_filter(query)\n",
    "\n",
    "    def polygon_within_multipol(self, coordinates:str, precision:int=5)->dict:\n",
    "        '''Precision is in meters. Returns the polygon in self.feature_name \n",
    "        which intersects a buffer of {precision} meters centralized at the point'''\n",
    "\n",
    "        query = f'DWITHIN(ge_multipoligono,POLYGON({coordinates}),{precision},meters)'\n",
    "        self.add_to_filter(query)\n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        \n",
    "        return self.cql_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22154223-d327-4fec-8d5d-62254ae69490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec25ada3-306d-4e9a-82d9-039d0ce0488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paginator:\n",
    "    def __init__(self, get_function: BaseClient, schemas: dict = None) -> None:\n",
    "        self.schemas = schemas or {}\n",
    "        self.get = get_function\n",
    "\n",
    "    def extract_total_features(self, resp: dict) -> int:\n",
    "        return resp[\"totalFeatures\"]\n",
    "\n",
    "    def extract_returned_quantity(self, resp: dict) -> int:\n",
    "        return len(resp[\"features\"])\n",
    "\n",
    "    def needs_pagination(self, total_features: int, returned_quantity: int) -> bool:\n",
    "        return total_features > returned_quantity\n",
    "\n",
    "    def get_steps(self, total_features: int, returned_quantity: int) -> list:\n",
    "        # need to start at zero because first query wans't ordered\n",
    "        return [step for step in range(0, total_features, returned_quantity)]\n",
    "\n",
    "    def warn_steps(self, feature_name: str, steps: list) -> None:\n",
    "        total_steps = len(steps)\n",
    "\n",
    "        warnings.warn(f\"Paginação iniciada. Serão realizadas {total_steps} requisições para a {feature_name}\")\n",
    "\n",
    "    def get_index_col(self, feature_name: str, index_col: str = None) -> str:\n",
    "        if index_col:\n",
    "            warnings.warn(f\"Certifique-se que a index_col de fato é uma coluna existente na feature\")\n",
    "            return index_col\n",
    "\n",
    "        if not self.schemas:\n",
    "            raise ValueError(f\"Must set schemas if willing to paginate and not specify index col\")\n",
    "\n",
    "        feature_schemas = self.schemas.get(feature_name, None)\n",
    "        if feature_schemas is None:\n",
    "            raise ValueError(f\"Schema for feature {feature_name} not found. Must specify index col\")\n",
    "\n",
    "        index_col = feature_schemas.get(\"id_col\")\n",
    "        if index_col is None:\n",
    "            raise ValueError(f\"Feature {feature_name} has no defaul index col. Must specify index col\")\n",
    "\n",
    "        return index_col\n",
    "\n",
    "    def paginate(self, feature_name: str, resp: dict, index_col: str = None, *_ignored, **query_params) -> dict:\n",
    "        total_features = self.extract_total_features(resp)\n",
    "        returned_quantity = self.extract_returned_quantity(resp)\n",
    "\n",
    "        if not self.needs_pagination(total_features, returned_quantity):\n",
    "            return resp\n",
    "\n",
    "        print(\"Paginação iniciada\")\n",
    "        index_col = self.get_index_col(feature_name, index_col)\n",
    "\n",
    "        steps = self.get_steps(total_features, returned_quantity)\n",
    "        self.warn_steps(feature_name, steps)\n",
    "\n",
    "        # recriando as features\n",
    "        resp[\"features\"] = []\n",
    "\n",
    "        for _ in steps:\n",
    "            resp_step = self.get(\"GetFeature\", typeName=feature_name, sortBy=index_col, **query_params)\n",
    "            features_step = resp_step[\"features\"]\n",
    "            resp[\"features\"].extend(features_step)\n",
    "            max_index = features_step[-1][\"properties\"][index_col]\n",
    "            query_params[\"cql_filter\"] = f\"{index_col}>{max_index}\"\n",
    "\n",
    "        total_returned_features = len(resp[\"features\"])\n",
    "        if not total_returned_features == total_features:\n",
    "            raise PaginationError(\n",
    "                f\"\"\"Difference in total features and paginated features: \n",
    "                          total - {total_features} vs returned - {total_returned_features}\n",
    "                          \"\"\"\n",
    "            )\n",
    "\n",
    "        return resp\n",
    "\n",
    "    def __call__(self, feature_name: str, resp: dict, index_col: str = None, *_ignored, **query_params) -> dict:\n",
    "        return self.paginate(feature_name, resp, index_col, **query_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90481101-53d9-4936-b655-a1d85033a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "class FeatureMdataParser:\n",
    "    '''Parses feature metadata'''\n",
    "\n",
    "    def parse_property(self, prop:dict)->dict:\n",
    "\n",
    "        name = prop['name']\n",
    "        parsed = dict(\n",
    "            nullable = prop['nillable'],\n",
    "            dtype = prop['localType']\n",
    "            )\n",
    "\n",
    "        return {name : parsed}\n",
    "    \n",
    "    def get_cd_identificador(self, prop:dict, cd_col_name:str='cd_identifica')->bool:\n",
    "\n",
    "        name = prop['name']\n",
    "        if name.lower().startswith(cd_col_name):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def set_cd_identificador(self, properties:dict, prop:dict)->None:\n",
    "\n",
    "        if self.get_cd_identificador(prop) and \\\n",
    "            not properties.get('id_col'):\n",
    "\n",
    "            properties['id_col'] = prop['name']\n",
    "\n",
    "    def parse_feature_schema(self, feat:dict)->dict:\n",
    "\n",
    "        name = feat['typeName']\n",
    "        properties = OrderedDict()\n",
    "        for prop in feat['properties']:\n",
    "            parsed_prop = self.parse_property(prop)\n",
    "            properties.update(parsed_prop)\n",
    "\n",
    "            self.set_cd_identificador(properties, prop)\n",
    "\n",
    "        parsed ={\n",
    "            name : properties\n",
    "        }\n",
    "\n",
    "        return parsed\n",
    "    \n",
    "    def raise_for_no_id(self, parsed_feature:dict)->None:\n",
    "\n",
    "        for feature_name, mdata in parsed_feature.items():\n",
    "            if 'id_col' not in mdata:\n",
    "                warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n",
    "\n",
    "    def parse_all_features(self, get_feature_resp:dict)->dict:\n",
    "\n",
    "        features = get_feature_resp['featureTypes']\n",
    "\n",
    "        parsed = OrderedDict()\n",
    "        for feat in features:\n",
    "            parsed_feat = self.parse_feature_schema(feat)\n",
    "            parsed.update(parsed_feat)\n",
    "            self.raise_for_no_id(parsed_feat)\n",
    "        return parsed\n",
    "\n",
    "    def __call__(self, get_feature_resp:dict)->dict:\n",
    "\n",
    "        return self.parse_all_features(get_feature_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2694f59f-6edd-42e5-8c0f-ddadf07053f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoSampaWfs(BaseClient):\n",
    "\n",
    "    def __init__(self, domain:str, set_schemas:bool=True, verbose:bool=True, auto_paginate:bool=True):\n",
    "\n",
    "        self.get = BaseClient(domain=domain, verbose=verbose)\n",
    "        self.parse_features_schemas = FeatureMdataParser()\n",
    "\n",
    "        self.schemas = self.get_feature_schemas() if set_schemas else None\n",
    "        self.paginate = Paginator(self.get, self.schemas)\n",
    "\n",
    "        self.auto_paginate = auto_paginate\n",
    "    \n",
    "    def __list_feature_types_raw(self):\n",
    "\n",
    "        return self.get('DescribeFeatureType')\n",
    "\n",
    "    def get_feature_schemas(self):\n",
    "\n",
    "        resp  = self.__list_feature_types_raw()\n",
    "        return self.parse_features_schemas(resp)\n",
    "\n",
    "    def __solve_properties(self, query_params, properties:list=None):\n",
    "\n",
    "         if properties:\n",
    "            names = ','.join(properties)\n",
    "            query_params['propertyName']=names\n",
    "\n",
    "    def __check_feature_exists(self, feature_name:str)->None:\n",
    "\n",
    "        if self.schemas and feature_name not in self.schemas:\n",
    "            raise ValueError(f\"Feature name {feature_name} doesn't exits\")\n",
    "\n",
    "    def get_feature(self, feature_name:str, properties:list=None, filter:CQLFilter=None, paginate:bool=None,\n",
    "                    index_col:str=None, **query_params):\n",
    "\n",
    "        self.__check_feature_exists(feature_name)        \n",
    "        self.__solve_properties(query_params, properties)\n",
    "       \n",
    "        if filter is not None:\n",
    "            query_params['cql_filter'] = filter()\n",
    "\n",
    "        resp = self.get('GetFeature', typeName=feature_name, **query_params)\n",
    "\n",
    "        if paginate or (paginate is None and self.auto_paginate):\n",
    "            return self.paginate(feature_name, resp, index_col, **query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff212b58-dc86-4436-a6f4-7ea5b49844e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b120f01-e418-49ff-9efd-589e1116c941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\General Projects\\Git\\GitHub\\sepep-pmsp\\saade_governancadados\n"
     ]
    }
   ],
   "source": [
    "#ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "print (ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98509da9-5334-4eed-8c6d-7afaa0781dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingEnvironmentVariable(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def get_dotenv_path(root: str = ROOT_DIR, example: bool = False) -> str:\n",
    "    \"\"\"Returns .env file path\"\"\"\n",
    "\n",
    "    path = os.path.join(root, \".env\")\n",
    "\n",
    "    if example:\n",
    "        path += \".example\"\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def dotenv_exits(dotenv_path: str = None, root: str = ROOT_DIR):\n",
    "    \"\"\"Check if .env file path exists\"\"\"\n",
    "\n",
    "    dotenv_path = dotenv_path or get_dotenv_path(root)\n",
    "\n",
    "    return os.path.exists(dotenv_path)\n",
    "\n",
    "\n",
    "def solve_dot_env(root: str = ROOT_DIR) -> str:\n",
    "    \"\"\"Copies .env.example as .env if .env doesnt exists\"\"\"\n",
    "\n",
    "    dotenv_path = get_dotenv_path(root)\n",
    "\n",
    "    if not dotenv_exits(dotenv_path):\n",
    "        warnings.warn(\".env file not found: copying .env.example\")\n",
    "        env_example = get_dotenv_path(root, example=True)\n",
    "        shutil.copy(env_example, dotenv_path)\n",
    "\n",
    "    return dotenv_path\n",
    "\n",
    "\n",
    "def load_env(root: str = ROOT_DIR) -> None:\n",
    "    \"\"\"Loads .env. If .env doesn't exists, will save .env.example\n",
    "    as .env and will load it's variables\"\"\"\n",
    "\n",
    "    env_path = solve_dot_env(root)\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "\n",
    "def load_var(varname: str, root: str = ROOT_DIR) -> str:\n",
    "    load_env(root)\n",
    "\n",
    "    try:\n",
    "        return os.environ[varname]\n",
    "    except KeyError:\n",
    "        raise MissingEnvironmentVariable(f\"Environment var {varname} not defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf8e65f-39a5-4f37-a09f-f61ab9a029ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOSAMPA_WFS_DOMAIN = load_var('GEOSAMPA_WFS_DOMAIN')\n",
    "\n",
    "def get_client(domain=GEOSAMPA_WFS_DOMAIN):\n",
    "    print (GEOSAMPA_WFS_DOMAIN)\n",
    "\n",
    "    return GeoSampaWfs(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "905b0ea1-5deb-4bf9-9583-08228a805e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://wfs.geosampa.prefeitura.sp.gov.br/geoserver/ows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_cortico_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_favela_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_loteamento_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_nucleo_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n"
     ]
    }
   ],
   "source": [
    "geosampa = get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7683c98e-e965-4625-b9e5-4914be74e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_cortico_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_favela_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_loteamento_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_15376\\2719722458.py:51: UserWarning: Could not identify id col for feature geohabisp_vw_wfs_nucleo_geosampa\n",
      "  warnings.warn(f\"Could not identify id col for feature {feature_name}\")\n"
     ]
    }
   ],
   "source": [
    "schemas = geosampa.get_feature_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4a523d0-3a02-4f5e-a91a-cd857009dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garantir que todas as camadas configuradas atualmente ainda existem no geosampa\n",
    "\n",
    "camadas_atuais = [\n",
    "    'GEOSAMPA_cadparcs_area_protecao_apa'\n",
    "    ,'area_contaminada_reabilitada_svma'\n",
    "    ,'patrimonio_cultural_area_envoltoria_CONDEPHAAT'\n",
    "    ,'patrimonio_cultural_area_envoltoria_CONPRESP'\n",
    "    ,'patrimonio_cultural_area_envoltoria_IPHAN'\n",
    "    ,'patrimonio_cultural_lugar_paisagistico_ambiental'\n",
    "    ,'calcada'\n",
    "    ,'classificacao_viaria_cet'\n",
    "    ,'decreto_utilidade_publica_interesse_social'\n",
    "    ,'operacao_urbana'\n",
    "    ,'GEOSAMPA_cadparcs_parque_estadual'\n",
    "    ,'GEOSAMPA_cadparcs_parque_outra_secretaria'\n",
    "    ,'requalifica_centro_perimetro_especial'\n",
    "    ,'requalifica_centro_perimetro_geral'\n",
    "    ,'area_risco_geologico'\n",
    "    ,'GEOSAMPA_cadparcs_reserva_particular_natural'\n",
    "    ,'patrimonio_cultural_sitio_arqueologico'\n",
    "    ,'termo_compromisso_ambiental_svma'\n",
    "    ,'GEOSAMPA_terra_indigena'\n",
    "    ,'patrimonio_cultural_bem_tombado'\n",
    "    ,'GEOSAMPA_cadparcs_unidade_conservacao_existente'\n",
    "    ,'zoneamento_2016_map1'\n",
    "    ,'restricao_geotecnica'\n",
    "    ,'remanescente_pmma'\n",
    "    ,'manancial_billings'\n",
    "    ,'manancial_guarapiranga'\n",
    "    ,'manancial_juquery'\n",
    "    ,'restricao_mirante_santana'\n",
    "    ,'linha_alta_tensao'\n",
    "    ,'transpetro_duto'\n",
    "    ,'geoconvias_faixa_nao_edificavel'\n",
    "    ,'geoconvias_lei_melhoramento_vigente'\n",
    "    ,'rampa_heliponto_licenciado'\n",
    "    ,'distrito_municipal'\n",
    "    ,'subprefeitura'\n",
    "    ,'distrito_municipal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d8f4ee1-a9d4-40e1-8f1c-6fda4fc8c313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOSAMPA_cadparcs_parque_estadual\n",
      "GEOSAMPA_cadparcs_parque_outra_secretaria\n",
      "GEOSAMPA_cadparcs_reserva_particular_natural\n",
      "GEOSAMPA_cadparcs_unidade_conservacao_existente\n",
      "rampa_heliponto_licenciado\n"
     ]
    }
   ],
   "source": [
    "valida_camadas_existentes = { cam_existente : (cam_existente in schemas.keys())  for cam_existente in camadas_atuais}\n",
    "print(\"\\n\".join([k for k, v in valida_camadas_existentes.items() if not v]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaa121b2-c5f9-45fc-a8f0-6e2cc92e31d2",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "for k, v in valida_camadas_existentes.items():\n",
    "    if v == False:\n",
    "        print(k)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06cbf67-5beb-437c-bea9-d9992357d343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geohabisp_vw_wfs_cortico_geosampa - Não tem atributo 'id_col' !!!\n",
      "geohabisp_vw_wfs_favela_geosampa - Não tem atributo 'id_col' !!!\n",
      "geohabisp_vw_wfs_loteamento_geosampa - Não tem atributo 'id_col' !!!\n",
      "geohabisp_vw_wfs_nucleo_geosampa - Não tem atributo 'id_col' !!!\n",
      "Total keys: 395 - Empty keys: 4\n"
     ]
    }
   ],
   "source": [
    "existe_lines = []\n",
    "dict_layer_id_col = {}\n",
    "dict_layer_schema = {}\n",
    "\n",
    "existe = False\n",
    "empty_id_cols = 0\n",
    "result_lines = []\n",
    "result = ''\n",
    "\n",
    "for camada in schemas.keys():\n",
    "    i = 0\n",
    "    existe = camada in camadas_atuais\n",
    "            \n",
    "    id_col = ''\n",
    "    if 'id_col' in schemas[camada]:\n",
    "        dict_layer_id_col[camada] = (schemas[camada])['id_col']\n",
    "        id_col = (schemas[camada])['id_col']\n",
    "    else:\n",
    "        dict_layer_id_col[camada] = None\n",
    "        print(f\"{camada} - Não tem atributo 'id_col' !!!\")\n",
    "        empty_id_cols = empty_id_cols + 1\n",
    "\n",
    "    for col in schemas[camada].keys():\n",
    "        if col == 'id_col':\n",
    "            continue\n",
    "            \n",
    "        key = 'Sim' if col == id_col else 'Não'        \n",
    "        prop_dict = (schemas[camada])[col]\n",
    "        result = '\\t'.join([camada, col, key, ('Sim' if prop_dict['nullable'] else 'Não'), prop_dict['dtype']])\n",
    "        if existe:\n",
    "            existe_lines.append(result)\n",
    "        result_lines.append((result)) \n",
    "        \n",
    "print(f\"Total keys: {len(dict_layer_id_col)} - Empty keys: {empty_id_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3bd11cf-2c72-4dc1-a7f4-16083f624ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "dh_arquivo = f\"{datetime.datetime.now():%Y%m%d_%H%M}\"\n",
    "\n",
    "log_fname = f\"{ROOT_DIR}/output/{dh_arquivo}_schema_keys.txt\"\n",
    "with open(log_fname, 'w') as sk:\n",
    "    sk.write(json.dumps(dict_layer_id_col))\n",
    "\n",
    "log_fname = f\"{ROOT_DIR}/output/{dh_arquivo}_schema_geosampa.txt\"\n",
    "with open(log_fname, 'w') as sg:\n",
    "    sg.write('\\n'.join(result_lines))\n",
    "\n",
    "log_fname = f\"{ROOT_DIR}/output/{dh_arquivo}_schema.txt\"\n",
    "with open(log_fname, 'w') as s:\n",
    "    s.write('\\n'.join(existe_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77fba353-5a9d-4565-a632-2d51b836cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento concluído: 30/41</2024 18:08\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processamento concluído: {datetime.datetime.now():%d/%M</%Y %H:%m}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
